__author__ = "Jumperkables"
"""
Generated by ChatGPT
"""
import os, sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import ImageFolder
import pytorch_lightning as pl

#os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
DATA_DIR = f"data/isic_subset_cleaned"
NUM_CLASSES = len(os.listdir(DATA_DIR))
NUM_HEADS = 4
BATCH_SIZE = 16
ORTHOGONALITY_INTENSITY = 0.2
SALIENCY_INTENSITY = 0.0002
SALIENCY_SIGMA = 1e-7
SALIENCY_ALPHA = 2e-8

class SkinLesionClassifier(pl.LightningModule):
    def __init__(self):
        super(SkinLesionClassifier, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        for x in range(NUM_HEADS):
            head = nn.Sequential(
                nn.Linear(512 * 14 * 14, 512),
                nn.ReLU(inplace=True),
                nn.Dropout(0.5),
                nn.Linear(512, NUM_CLASSES),
            )
            setattr(self, f"head_{x}", head)
            #setattr(self, f"criterion_{x}", nn.CrossEntropyLoss())
        self.criterion = nn.CrossEntropyLoss()



    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        head_outputs = []
        for i in range(NUM_HEADS):
            h_out = getattr(self, f"head_{i}")(x)
            head_outputs.append(h_out)
        return torch.stack(head_outputs, dim=0)



    def exec_step(self, mode, batch, batch_idx):
        images, labels = batch
        images.requires_grad_()
        head_outputs = self(images)
        dot_prods = []  # Calculate vector similarity, and penalise to enforce orthogonality 
        saliencies = [] # Make sure that saliency maps are relatively concentrated
        for i in range(NUM_HEADS):
            if mode == "train": 
                head_outputs[i].max(dim=1).values.sum().backward(retain_graph=True)
                grads = images.grad.data
                mean_mat = torch.ones(images.shape).to(images.device)*grads.mean()
                diffs = (grads-mean_mat).abs()
                sal = SALIENCY_ALPHA/(((SALIENCY_INTENSITY*diffs.mean())*0.5)+SALIENCY_SIGMA)
                saliencies.append(sal)
            else:
                saliencies.append(0.)
            for j in range(NUM_HEADS):
                if i != j:
                    dim_0 = head_outputs[i].shape[0]
                    dim_1 = head_outputs[i].shape[1]
                    dp = torch.bmm( head_outputs[i].view(dim_0, 1, dim_1), head_outputs[j].view(dim_0, dim_1, 1) )
                    dp = dp.squeeze(1).squeeze(1).abs()
                    dot_prods.append(dp)
        dot_prods = torch.stack(dot_prods, dim=0)
        head_loss = self.criterion(head_outputs.mean(dim=0), labels) # Standard loss for class predictions
        orthog_loss = dot_prods.mean() # Penalise vector similarity, enforce orthogonality
        loss = head_loss + orthog_loss
        saliency_loss = sum(saliencies)
        loss = loss + saliency_loss
        self.log(f"{mode}_loss_head", head_loss, prog_bar=(True if mode == "train" else False))
        self.log(f"{mode}_loss_orthog", orthog_loss, prog_bar=(True if mode == "train" else False))
        self.log(f"{mode}_loss_saliency", saliency_loss, prog_bar=(True if mode == "train" else False))
        self.log(f"{mode}_loss_total", loss)
        return loss



    def training_step(self, batch, batch_idx):
        loss = self.exec_step("train", batch, batch_idx)
        return loss



    def validation_step(self, batch, batch_idx):
        loss = self.exec_step("valid", batch, batch_idx)



    def configure_optimizers(self):
        return optim.Adam(self.parameters(), lr=0.001)



# Set transforms for data augmentation
data_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the dataset
dataset = ImageFolder(DATA_DIR, transform=data_transforms)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# Initialize the Lightning module
skin_classifier = SkinLesionClassifier()

# Create a PyTorch Lightning trainer
trainer = pl.Trainer(max_epochs=10, num_nodes=1, accelerator="gpu") #gpus=1 if torch.cuda.is_available() else 0)

# Train the model
trainer.fit(skin_classifier, train_loader, test_loader)
